{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19214eb1-91c7-479b-b9a4-80cf21618a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: opencv-python in c:\\users\\ben.magrathea\\.pyenv\\pyenv-win\\versions\\3.9.0\\lib\\site-packages (4.7.0.68)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\ben.magrathea\\.pyenv\\pyenv-win\\versions\\3.9.0\\lib\\site-packages (from opencv-python) (1.24.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a73483-42a9-4255-a6da-deb51f19d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_your_data():\n",
    "    sift = cv2.SIFT_create()\n",
    "    data_path = 'data'\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "    \n",
    "    action_map = {\n",
    "        'boxing': 0,\n",
    "        'handclapping': 1,\n",
    "        'handwaving': 2,\n",
    "        'jogging': 3,\n",
    "        'running': 4,\n",
    "        'walking': 5\n",
    "    }\n",
    "\n",
    "    for action in os.listdir(data_path):\n",
    "        if action not in action_map:\n",
    "            continue\n",
    "        action_path = os.path.join(data_path, action)\n",
    "        for person in os.listdir(action_path):\n",
    "            person_path = os.path.join(action_path, person)\n",
    "            for video in os.listdir(person_path):\n",
    "                video_path = os.path.join(person_path, video)\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                features = []\n",
    "                ret, prev_frame = cap.read()\n",
    "                prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "                mhi = np.zeros_like(prev_frame, dtype=np.float32)\n",
    "                \n",
    "                while True:\n",
    "                    ret, current_frame = cap.read()\n",
    "                    \n",
    "                    if not ret:\n",
    "                        break\n",
    "                    \n",
    "                    current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "                    mhi = motion_recognition.update_mhi(prev_frame, current_frame_gray, mhi)\n",
    "                    keypoints, descriptors = motion_recognition.extract_sift_features(current_frame)\n",
    "                    features.append(descriptors)\n",
    "                    prev_frame = current_frame_gray\n",
    "                \n",
    "                cap.release()\n",
    "                \n",
    "                if len(features) < 10:\n",
    "                    # Discard videos that are too short\n",
    "                    continue\n",
    "                \n",
    "                # Split the video features into training and testing sets\n",
    "                split_index = int(len(features) * 0.8)\n",
    "                train_data.extend(features[:split_index])\n",
    "                test_data.extend(features[split_index:])\n",
    "                train_labels.extend([action_map[action]] * split_index)\n",
    "                test_labels.extend([action_map[action]] * (len(features) - split_index))\n",
    "                \n",
    "    X_train = np.vstack(train_data)\n",
    "    y_train = np.array(train_labels)\n",
    "    X_test = np.vstack(test_data)\n",
    "    y_test = np.array(test_labels)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138aa142-f79d-4669-a4b5-6f21e2bdd6e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[39mreturn\u001b[39;00m scores\u001b[39m.\u001b[39mmean(), scores\u001b[39m.\u001b[39mstd()\n\u001b[0;32m     70\u001b[0m \u001b[39m# Load your training and testing data here\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m X_train, y_train, X_test, y_test \u001b[39m=\u001b[39m load_your_data()\n\u001b[0;32m     73\u001b[0m motion_recognition \u001b[39m=\u001b[39m MotionRecognition()\n\u001b[0;32m     75\u001b[0m \u001b[39m# Train the SVM classifier\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m, in \u001b[0;36mload_your_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m test_labels \u001b[39m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m action_map \u001b[39m=\u001b[39m {\n\u001b[0;32m     14\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mboxing\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mhandclapping\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mwalking\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5\u001b[39m\n\u001b[0;32m     20\u001b[0m }\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfor\u001b[39;00m action \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(data_path):\n\u001b[0;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m action \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m action_map:\n\u001b[0;32m     24\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/data'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "class MotionRecognition:\n",
    "    def __init__(self):\n",
    "        self.sift = cv2.SIFT_create()\n",
    "        self.mhi_duration = 10\n",
    "\n",
    "    def extract_sift_features(self, image):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        keypoints, descriptors = self.sift.detectAndCompute(gray, None)\n",
    "        return keypoints, descriptors\n",
    "\n",
    "    def update_mhi(self, prev_frame, current_frame, mhi):\n",
    "        diff = cv2.absdiff(prev_frame, current_frame)\n",
    "        _, binary_diff = cv2.threshold(diff, 30, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "        timestamp = cv2.getTickCount() / cv2.getTickFrequency()\n",
    "        mhi = cv2.motempl.updateMotionHistory(binary_diff, mhi, timestamp, self.mhi_duration)\n",
    "\n",
    "        return mhi\n",
    "\n",
    "    def process_video(self):\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        ret, prev_frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            raise ValueError(\"Cannot read the video file.\")\n",
    "\n",
    "        prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        mhi = np.zeros_like(prev_frame, dtype=np.float32)\n",
    "\n",
    "        while True:\n",
    "            ret, current_frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "            mhi = self.update_mhi(prev_frame, current_frame_gray, mhi)\n",
    "\n",
    "            keypoints, descriptors = self.extract_sift_features(current_frame)\n",
    "            current_frame_with_keypoints = cv2.drawKeypoints(current_frame, keypoints, None)\n",
    "\n",
    "            cv2.imshow(\"Current Frame with SIFT keypoints\", current_frame_with_keypoints)\n",
    "            cv2.imshow(\"MHI\", mhi)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            prev_frame = current_frame_gray\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def train_svm(self, X_train, y_train):\n",
    "        self.clf.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.clf.predict(X_test)\n",
    "\n",
    "    def k_fold_cross_validation(self, X, y, n_splits=5):\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "        scores = cross_val_score(self.clf, X, y, cv=kf)\n",
    "        return scores.mean(), scores.std()\n",
    "\n",
    "# Load your training and testing data here\n",
    "X_train, y_train, X_test, y_test = load_your_data()\n",
    "\n",
    "motion_recognition = MotionRecognition()\n",
    "\n",
    "# Train the SVM classifier\n",
    "motion_recognition.train_svm(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = motion_recognition.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier using k-fold cross-validation\n",
    "mean_accuracy, std_accuracy = motion_recognition.k_fold_cross_validation(X_train, y_train)\n",
    "print(f\"Mean accuracy: {mean_accuracy}, Standard deviation: {std_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f2b8a-e15f-47ad-a0ea-35514a577ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
